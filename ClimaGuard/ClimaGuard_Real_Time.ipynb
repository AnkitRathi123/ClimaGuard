{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52280f9b-98a9-4d64-ba95-cfabb78ccb68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Air Temperature**<br>\n",
    "readingType: DBT 1M F<br>\n",
    "readingUnit: deg code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b5bc736-d6c5-4362-a86d-5b3225ebb953",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Varaibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65a3200b-3db9-4664-ac5d-ca3122b2ae28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "today_str = date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "Date = today_str\n",
    "Datamode='append'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4435d37a-bb0b-4d0c-aab7-b1c0e99163bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "import requests\n",
    "\n",
    "# Fetch data from API\n",
    "url = f\"https://api-open.data.gov.sg/v2/real-time/api/air-temperature?date={Date}\"\n",
    "response = requests.get(url)\n",
    "data = response.json()[\"data\"]\n",
    "\n",
    "# Create Spark DataFrames\n",
    "stations_df = spark.createDataFrame(data[\"stations\"])\n",
    "\n",
    "timestamp = data[\"readings\"][0][\"timestamp\"]\n",
    "readings_data = data[\"readings\"][0][\"data\"]\n",
    "\n",
    "# ðŸ‘‡ Cast value to DoubleType to avoid merge error\n",
    "readings_df = spark.createDataFrame([(r['stationId'], float(r['value'])) for r in readings_data], schema=['stationId', 'value']) \\\n",
    "    .withColumn(\"timestamp\", F.lit(timestamp)) \\\n",
    "    .withColumn(\"value\", F.col(\"value\").cast(DoubleType()))\n",
    "\n",
    "# Join and select final columns\n",
    "final_df = stations_df.join(readings_df, stations_df.deviceId == readings_df.stationId, \"inner\") \\\n",
    "    .select(\n",
    "        readings_df.stationId.alias(\"deviceId\"),\n",
    "        stations_df.name,\n",
    "        stations_df.location[\"latitude\"].alias(\"latitude\"),\n",
    "        stations_df.location[\"longitude\"].alias(\"longitude\"),\n",
    "        readings_df.value,\n",
    "        readings_df.timestamp.cast(\"timestamp\")\n",
    "    )\n",
    "\n",
    "# Save to Delta table\n",
    "final_df.write.format(\"delta\").mode(Datamode).saveAsTable(\"climaGuard.singapore.air_temperature\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ab566f3-d5ed-4e57-a686-c423b5d7a5cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86a595ac-8721-4e53-a9dd-357dee88431e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Rainfall**<br>\n",
    "readingType: TB1 Rainfall 5 Minute Total F<br>\n",
    "readingUnit: mm<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6908efaa-e5d0-4950-959b-0457d95a0461",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, ArrayType, TimestampType\n",
    "\n",
    "# Define JSON response (you can replace this with actual API call using requests.get(...).json())\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = f\"https://api-open.data.gov.sg/v2/real-time/api/rainfall?date={Date}\"\n",
    "response = requests.get(url)\n",
    "data = response.json()[\"data\"]\n",
    "\n",
    "# Create DataFrames from 'stations' and 'readings'\n",
    "stations_df = spark.createDataFrame(data[\"stations\"])\n",
    "\n",
    "# Explode the readings array and extract timestamp\n",
    "timestamp = data[\"readings\"][0][\"timestamp\"]\n",
    "readings_data = data[\"readings\"][0][\"data\"]\n",
    "\n",
    "readings_df = spark.createDataFrame([(r['stationId'], float(r['value'])) for r in readings_data], schema=['stationId', 'value']) \\\n",
    "    .withColumn(\"timestamp\", F.lit(timestamp)) \\\n",
    "    .withColumn(\"reading_value\", F.col(\"value\").cast(DoubleType()))\n",
    "\n",
    "# Join stations with readings using stationId == deviceId\n",
    "final_df = stations_df.join(readings_df, stations_df.deviceId == readings_df.stationId, \"inner\") \\\n",
    "    .select(\n",
    "        readings_df.stationId.alias(\"deviceId\"),\n",
    "        stations_df.name,\n",
    "        stations_df.location[\"latitude\"].alias(\"latitude\"),\n",
    "        stations_df.location[\"longitude\"].alias(\"longitude\"),\n",
    "        readings_df.value,\n",
    "        readings_df.timestamp.cast(\"timestamp\")\n",
    "    )\n",
    "\n",
    "# Save as Delta Table in Databricks\n",
    "final_df.write.format(\"delta\").mode(Datamode).saveAsTable(\"climaGuard.singapore.rainfall\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "357a4b52-e8f4-4e3c-8112-3b574f2b38fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Relative Humidity**</br>\n",
    "readingType: RH 1M F<br>\n",
    "readingUnit: percentage<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0039791-ef93-468f-b079-6d82b01cddc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, ArrayType, TimestampType\n",
    "\n",
    "# Define JSON response (you can replace this with actual API call using requests.get(...).json())\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = f\"https://api-open.data.gov.sg/v2/real-time/api/relative-humidity?date={Date}\"\n",
    "response = requests.get(url)\n",
    "data = response.json()[\"data\"]\n",
    "\n",
    "# Create DataFrames from 'stations' and 'readings'\n",
    "stations_df = spark.createDataFrame(data[\"stations\"])\n",
    "\n",
    "# Explode the readings array and extract timestamp\n",
    "timestamp = data[\"readings\"][0][\"timestamp\"]\n",
    "readings_data = data[\"readings\"][0][\"data\"]\n",
    "\n",
    "readings_df = spark.createDataFrame([(r['stationId'], float(r['value'])) for r in readings_data], schema=['stationId', 'value']) \\\n",
    "    .withColumn(\"timestamp\", F.lit(timestamp)) \\\n",
    "    .withColumn(\"value\", F.col(\"value\").cast(DoubleType()))\n",
    "\n",
    "# Join stations with readings using stationId == deviceId\n",
    "final_df = stations_df.join(readings_df, stations_df.deviceId == readings_df.stationId, \"inner\") \\\n",
    "    .select(\n",
    "        readings_df.stationId.alias(\"deviceId\"),\n",
    "        stations_df.name,\n",
    "        stations_df.location[\"latitude\"].alias(\"latitude\"),\n",
    "        stations_df.location[\"longitude\"].alias(\"longitude\"),\n",
    "        readings_df.value,\n",
    "        readings_df.timestamp.cast(\"timestamp\")\n",
    "    )\n",
    "\n",
    "# Save as Delta Table in Databricks\n",
    "final_df.write.format(\"delta\").mode(Datamode).saveAsTable(\"climaGuard.singapore.relative_humidity\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92a3df99-5444-4a77-b160-c094f3f7410d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Wind Direction**</br>\n",
    "readingType: Wind Dir AVG (S) 10M M1M</br>\n",
    "readingUnit: degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0057bd4-3fa2-40eb-9861-6441a02a7b16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, ArrayType, TimestampType\n",
    "\n",
    "# Define JSON response (you can replace this with actual API call using requests.get(...).json())\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = f\"https://api-open.data.gov.sg/v2/real-time/api/wind-direction?date={Date}\"\n",
    "response = requests.get(url)\n",
    "data = response.json()[\"data\"]\n",
    "\n",
    "# Create DataFrames from 'stations' and 'readings'\n",
    "stations_df = spark.createDataFrame(data[\"stations\"])\n",
    "\n",
    "# Explode the readings array and extract timestamp\n",
    "timestamp = data[\"readings\"][0][\"timestamp\"]\n",
    "readings_data = data[\"readings\"][0][\"data\"]\n",
    "\n",
    "readings_df = spark.createDataFrame(readings_data).withColumn(\"timestamp\", F.lit(timestamp))\n",
    "\n",
    "# Join stations with readings using stationId == deviceId\n",
    "final_df = stations_df.join(readings_df, stations_df.deviceId == readings_df.stationId, \"inner\") \\\n",
    "    .select(\n",
    "        readings_df.stationId.alias(\"deviceId\"),\n",
    "        stations_df.name,\n",
    "        stations_df.location[\"latitude\"].alias(\"latitude\"),\n",
    "        stations_df.location[\"longitude\"].alias(\"longitude\"),\n",
    "        readings_df.value,\n",
    "        readings_df.timestamp.cast(\"timestamp\")\n",
    "    )\n",
    "\n",
    "# Save as Delta Table in Databricks\n",
    "final_df.write.format(\"delta\").mode(Datamode).saveAsTable(\"climaGuard.singapore.wind_direction\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da8600a3-3047-40c8-bc9d-72e1d94fe93a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Wind Speed**</br>\n",
    "readingType: Wind Speed AVG(S)10M M1M</br>\n",
    "readingUnit: knots</br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c181d635-fabb-4d16-8e79-af28db52af32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, ArrayType, TimestampType\n",
    "\n",
    "# Define JSON response (you can replace this with actual API call using requests.get(...).json())\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = f\"https://api-open.data.gov.sg/v2/real-time/api/wind-speed?date={Date}\"\n",
    "response = requests.get(url)\n",
    "data = response.json()[\"data\"]\n",
    "\n",
    "# Create DataFrames from 'stations' and 'readings'\n",
    "stations_df = spark.createDataFrame(data[\"stations\"])\n",
    "\n",
    "# Explode the readings array and extract timestamp\n",
    "timestamp = data[\"readings\"][0][\"timestamp\"]\n",
    "readings_data = data[\"readings\"][0][\"data\"]\n",
    "\n",
    "readings_df = spark.createDataFrame([(r['stationId'], float(r['value'])) for r in readings_data], schema=['stationId', 'value']) \\\n",
    "    .withColumn(\"timestamp\", F.lit(timestamp)) \\\n",
    "    .withColumn(\"value\", F.col(\"value\").cast(DoubleType()))\n",
    "\n",
    "# Join stations with readings using stationId == deviceId\n",
    "final_df = stations_df.join(readings_df, stations_df.deviceId == readings_df.stationId, \"inner\") \\\n",
    "    .select(\n",
    "        readings_df.stationId.alias(\"deviceId\"),\n",
    "        stations_df.name,\n",
    "        stations_df.location[\"latitude\"].alias(\"latitude\"),\n",
    "        stations_df.location[\"longitude\"].alias(\"longitude\"),\n",
    "        readings_df.value,\n",
    "        readings_df.timestamp.cast(\"timestamp\")\n",
    "    )\n",
    "\n",
    "# Save as Delta Table in Databricks\n",
    "final_df.write.format(\"delta\").mode(Datamode).saveAsTable(\"climaGuard.singapore.wind_speed\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ClimaGuard_Real_Time",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
